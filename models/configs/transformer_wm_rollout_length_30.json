{
    "input_size": 23,
    "embedding_size": 512,
    "num_heads": 4,
    "encoder_hidden_size": 512,
    "num_encoder_layers": 2,
    "num_decoder_layers": 2,
    "decoder_hidden_size": 256,
    "context_length": 50,
    "rollout_length": 30,
    "model_type": "transformer_wm"
}